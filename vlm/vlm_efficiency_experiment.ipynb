{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECE1512 Project A - Part B / Section 5\n",
        "## CLIP (ViT-B/16) Vision Token Pruning - Efficiency Toy Profiling"
      ],
      "metadata": {
        "id": "1W9pEu7ujjJe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxsBriCSjdcX"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers==4.44.2 timm==1.0.9\n",
        "\n",
        "import os, time, csv, math, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load CLIP Vision & Define Pruning Helpers"
      ],
      "metadata": {
        "id": "fETetOSrjsKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
        "\n",
        "VISION_ID = \"openai/clip-vit-base-patch16\"\n",
        "vision: CLIPVisionModel = CLIPVisionModel.from_pretrained(VISION_ID).to(DEVICE).eval()\n",
        "processor = CLIPImageProcessor.from_pretrained(VISION_ID)\n",
        "\n",
        "# ===========================================\n",
        "# Synthetic batch of images (efficiency focus)\n",
        "# You can swap to real images later if desired\n",
        "# ===========================================\n",
        "BATCH = 8      # images per batch\n",
        "RES   = 224    # CLIP default\n",
        "DUMMY_IMAGES = torch.randn(BATCH, 3, RES, RES, device=DEVICE)  # synthetic images\n",
        "\n",
        "# ===========================================\n",
        "# Patch embedding -> [B, N, D] tokens (no class/pos)\n",
        "# ===========================================\n",
        "def get_patch_tokens(model: CLIPVisionModel, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns patch tokens BEFORE adding class/pos embeddings.\n",
        "    Shape: [B, N, D]\n",
        "    \"\"\"\n",
        "    vm = model.vision_model\n",
        "    # Patch embedding: Conv2d on (B,3,H,W) -> (B,Hidden,H',W')\n",
        "    x = vm.embeddings.patch_embedding(pixel_values)\n",
        "    # Flatten to tokens\n",
        "    x = x.flatten(2).transpose(1, 2)  # [B, N, D]\n",
        "    return x\n",
        "\n",
        "# ===========================================\n",
        "# Token pruning by L2-norm scoring\n",
        "# Keep ratio p in (0,1]\n",
        "# ===========================================\n",
        "def prune_tokens(x_tokens: torch.Tensor, keep_ratio: float = 0.7):\n",
        "    \"\"\"\n",
        "    x_tokens: [B, N, D]\n",
        "    Returns:\n",
        "      x_pruned: [B, 1+K, D]  (we prepend a summary token)\n",
        "    \"\"\"\n",
        "    B, N, D = x_tokens.shape\n",
        "    K = max(1, int(N * keep_ratio))\n",
        "    scores = x_tokens.norm(dim=-1)                       # [B, N]\n",
        "    top_idx = scores.topk(K, dim=1).indices              # [B, K]\n",
        "    x_kept = x_tokens.gather(1, top_idx.unsqueeze(-1).expand(-1, -1, D))  # [B, K, D]\n",
        "    summary = x_tokens.mean(dim=1, keepdim=True)         # [B, 1, D]\n",
        "    x_pruned = torch.cat([summary, x_kept], dim=1)        # [B, 1+K, D]\n",
        "    return x_pruned\n",
        "\n",
        "# ===========================================\n",
        "# Baseline forward: use full vision forward()\n",
        "# ===========================================\n",
        "@torch.no_grad()\n",
        "def forward_baseline(model: CLIPVisionModel, pixel_values: torch.Tensor):\n",
        "    # Full forward (includes embeddings + encoder)\n",
        "    return model(pixel_values=pixel_values)\n",
        "\n",
        "# ===========================================\n",
        "# Pruned forward: run encoder on reduced sequence\n",
        "# (Optionally add sliced positional embeddings for better approximation)\n",
        "# IMPORTANT: pass hidden_states as POSitional arg (no kw) to avoid API error\n",
        "# ===========================================\n",
        "@torch.no_grad()\n",
        "def forward_pruned_encoder_only(model: CLIPVisionModel, x_pruned: torch.Tensor, add_positional: bool = True):\n",
        "    \"\"\"\n",
        "    Run the CLIP vision encoder directly on a reduced token sequence.\n",
        "    Optionally add sliced positional embeddings to better approximate ViT inputs.\n",
        "    Returns: last hidden states [B, L, D]\n",
        "    \"\"\"\n",
        "    vm = model.vision_model  # CLIPVisionTransformer\n",
        "\n",
        "    hidden_states = x_pruned  # [B, L, D] on correct device/dtype already\n",
        "\n",
        "    # (Optional) add positional embeddings sliced to current token length\n",
        "    if add_positional and hasattr(vm, \"embeddings\") and hasattr(vm.embeddings, \"position_embedding\"):\n",
        "        # vm.embeddings.position_embedding: [1, num_patches+1, D]\n",
        "        pos = vm.embeddings.position_embedding.weight[: hidden_states.size(1), :].unsqueeze(0).to(hidden_states.device).to(hidden_states.dtype)\n",
        "        hidden_states = hidden_states + pos\n",
        "\n",
        "    # Some CLIP variants apply a pre-layernorm before encoder; if present, use it.\n",
        "    if hasattr(vm, \"pre_layrnorm\") and vm.pre_layrnorm is not None:\n",
        "        hidden_states = vm.pre_layrnorm(hidden_states)\n",
        "\n",
        "    # Pass as positional arg (no keyword 'hidden_states')\n",
        "    out = vm.encoder(\n",
        "        hidden_states,                 # positional, not 'hidden_states=...'\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "    # Some models apply a final layernorm after encoder; keep it if available.\n",
        "    if hasattr(vm, \"post_layernorm\") and vm.post_layernorm is not None:\n",
        "        return vm.post_layernorm(out.last_hidden_state)\n",
        "\n",
        "    return out.last_hidden_state"
      ],
      "metadata": {
        "id": "gDmh6XRgjsY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measurement & Run the Sweep (p = 1.0, 0.9, 0.7, 0.5)"
      ],
      "metadata": {
        "id": "5ArgoL5bj7ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_WARMUP = 3\n",
        "N_RUNS   = 10\n",
        "\n",
        "def cuda_sync():\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency_mem(fn, *args, **kwargs):\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # warm-up\n",
        "    for _ in range(N_WARMUP):\n",
        "        _ = fn(*args, **kwargs)\n",
        "        cuda_sync()\n",
        "\n",
        "    times = []\n",
        "    for _ in range(N_RUNS):\n",
        "        t0 = time.time()\n",
        "        _ = fn(*args, **kwargs)\n",
        "        cuda_sync()\n",
        "        times.append(time.time() - t0)\n",
        "\n",
        "    lat_ms = float(np.mean(times) * 1000.0)\n",
        "    thr = (BATCH) / np.mean(times)\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / (1024**2) if DEVICE == \"cuda\" else float('nan')\n",
        "    return lat_ms, thr, peak_mem\n",
        "\n",
        "# ===========================================\n",
        "# Build pixel values from synthetic images\n",
        "# (We pass raw tensors; processor is not strictly needed for random data)\n",
        "# ===========================================\n",
        "pixel_values = DUMMY_IMAGES  # already normalized-ish random\n",
        "\n",
        "# Baseline patch tokens for pruning reference\n",
        "with torch.no_grad():\n",
        "    tokens_full = get_patch_tokens(vision, pixel_values)  # [B, N, D]\n",
        "\n",
        "# ===========================================\n",
        "# Run sweep over keep ratios\n",
        "# ===========================================\n",
        "keep_ratios = [1.0, 0.9, 0.7, 0.5]  # 100%, 90%, 70%, 50% tokens kept\n",
        "results = []\n",
        "\n",
        "# 1) Baseline (full forward)\n",
        "lat, thr, mem = measure_latency_mem(forward_baseline, vision, pixel_values)\n",
        "results.append((\"Baseline\", 1.0, lat, thr, mem))\n",
        "\n",
        "# 2) Pruned variants (encoder-only on reduced sequence)\n",
        "for p in keep_ratios:\n",
        "    if p == 1.0:\n",
        "        # For reporting consistency, also compute \"pruned at p=1.0\"\n",
        "        x_pruned = torch.cat([tokens_full.mean(dim=1, keepdim=True), tokens_full], dim=1)\n",
        "    else:\n",
        "        x_pruned = prune_tokens(tokens_full, keep_ratio=p)\n",
        "    lat, thr, mem = measure_latency_mem(forward_pruned_encoder_only, vision, x_pruned)\n",
        "    results.append((f\"VTP p={p:.1f}\", p, lat, thr, mem))\n",
        "\n",
        "# ===========================================\n",
        "# Save CSV\n",
        "# ===========================================\n",
        "os.makedirs(\"vlm/results\", exist_ok=True)\n",
        "csv_path = \"vlm/results/clip_vtp_results.csv\"\n",
        "with open(csv_path, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"Variant\", \"KeepRatio\", \"Latency(ms)\", \"Throughput(img/s)\", \"PeakMem(MiB)\"])\n",
        "    for r in results:\n",
        "        w.writerow([r[0], f\"{r[1]:.2f}\", f\"{r[2]:.3f}\", f\"{r[3]:.3f}\", f\"{r[4]:.1f}\"])\n",
        "print(\"Saved CSV ->\", csv_path)\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n==== Results ====\")\n",
        "print(\"{:<12} {:>10} {:>14} {:>18} {:>14}\".format(\"Variant\",\"KeepRatio\",\"Latency(ms)\",\"Throughput(img/s)\",\"PeakMem(MiB)\"))\n",
        "for lab, p, lat, thr, mem in results:\n",
        "    print(\"{:<12} {:>10.2f} {:>14.2f} {:>18.2f} {:>14.1f}\".format(lab, p, lat, thr, mem))"
      ],
      "metadata": {
        "id": "7S8m285xj7nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots (Latency & Memory)"
      ],
      "metadata": {
        "id": "q4JicHbNkAZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [r[0] for r in results]\n",
        "latencies = [r[2] for r in results]\n",
        "mems = [r[4] for r in results]\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels, latencies)\n",
        "plt.ylabel(\"Latency (ms)\")\n",
        "plt.title(\"CLIP VTP — Latency\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "lat_fig = \"vlm/results/clip_vtp_latency.png\"\n",
        "plt.savefig(lat_fig, dpi=150)\n",
        "print(\"Saved ->\", lat_fig)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels, mems)\n",
        "plt.ylabel(\"Peak Memory (MiB)\")\n",
        "plt.title(\"CLIP VTP — Peak Memory\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "mem_fig = \"vlm/results/clip_vtp_memory.png\"\n",
        "plt.savefig(mem_fig, dpi=150)\n",
        "print(\"Saved ->\", mem_fig)"
      ],
      "metadata": {
        "id": "XJ7lWrx2kC_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy test"
      ],
      "metadata": {
        "id": "nhPm5y-ggwVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "ey1I77Iqg7OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# CLIP Zero-shot Accuracy on CIFAR-10\n",
        "#   - Baseline: full CLIP (no pruning)\n",
        "#   - VTP: use your own pruning path (get_patch_tokens + prune_tokens +\n",
        "#         forward_pruned_encoder_only) to get image features, then reuse\n",
        "#         CLIP text encoder + visual_projection + logit_scale.\n",
        "# ===========================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "# ---- 1. Load full CLIP (for text + projection) ----\n",
        "CLIP_ID = VISION_ID  # \"openai/clip-vit-base-patch16\"\n",
        "clip_model = CLIPModel.from_pretrained(CLIP_ID).to(DEVICE).eval()\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_ID)\n",
        "\n",
        "# ---- 2. CIFAR-10 labels + zero-shot prompts ----\n",
        "classes = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "text_inputs = clip_processor(\n",
        "    text=[f\"a photo of a {c}\" for c in classes],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(DEVICE)\n",
        "\n",
        "# Precompute text features (shared across all runs)\n",
        "with torch.no_grad():\n",
        "    text_outputs = clip_model.text_model(**text_inputs)\n",
        "    text_embeds = text_outputs.last_hidden_state[:, 0, :]      # CLS token\n",
        "    text_features = clip_model.text_projection(text_embeds)    # [10, D']\n",
        "    text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "# ---- 3. Load a subset of CIFAR-10 ----\n",
        "dataset = load_dataset(\"cifar10\", split=\"test[:100]\")\n",
        "print(\"Loaded CIFAR-10 subset size:\", len(dataset))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_accuracy_baseline_cifar(dataset):\n",
        "    \"\"\"\n",
        "    Baseline: full CLIP (no pruning), standard zero-shot.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for sample in dataset:\n",
        "        img = sample[\"img\"]\n",
        "        label = sample[\"label\"]\n",
        "\n",
        "        inputs = clip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
        "        pixel_values = inputs[\"pixel_values\"]  # [1, 3, H, W]\n",
        "\n",
        "        # Full CLIP forward (image + text)\n",
        "        outputs = clip_model(\n",
        "            pixel_values=pixel_values,\n",
        "            **text_inputs\n",
        "        )\n",
        "        logits_per_image = outputs.logits_per_image  # [1, 10]\n",
        "\n",
        "        pred = logits_per_image.argmax(dim=-1).item()\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_accuracy_vtp_cifar(dataset, keep_ratio: float):\n",
        "    \"\"\"\n",
        "    VTP: use YOUR pruning path for the image encoder.\n",
        "\n",
        "    Pipeline:\n",
        "      1) use your `get_patch_tokens` to get patch tokens\n",
        "      2) use your `prune_tokens` to select summary + Top-K patches\n",
        "      3) run them through your `forward_pruned_encoder_only`\n",
        "      4) take CLS (index 0) as image embedding\n",
        "      5) project with CLIP visual_projection + logit_scale and compare with\n",
        "         precomputed text_features\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for sample in dataset:\n",
        "        img = sample[\"img\"]\n",
        "        label = sample[\"label\"]\n",
        "\n",
        "        # Use CLIP's image processor to get pixel_values (same as baseline)\n",
        "        inputs = clip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
        "        pixel_values = inputs[\"pixel_values\"]  # [1, 3, H, W]\n",
        "\n",
        "        # 1) patch tokens from your vision model\n",
        "        tokens = get_patch_tokens(vision, pixel_values)  # [B, N, D]\n",
        "\n",
        "        # 2) prune using your L2-norm rule (summary + top-K)\n",
        "        x_pruned = prune_tokens(tokens, keep_ratio=keep_ratio)  # [B, 1+K, D]\n",
        "\n",
        "        # 3) run your encoder-only pruned forward\n",
        "        #    NOTE: we recommend add_positional=False here, because your current\n",
        "        #    implementation of sliced positional embeddings is incomplete.\n",
        "        vision_output_hs = forward_pruned_encoder_only(\n",
        "            vision, x_pruned, add_positional=False\n",
        "        )  # [B, 1+K, D]\n",
        "\n",
        "        # 4) CLS token (index 0) as image embedding\n",
        "        image_embeds = vision_output_hs[:, 0, :]  # [B, D]\n",
        "\n",
        "        # 5) project to joint space and compute logits with text_features\n",
        "        image_features = clip_model.visual_projection(image_embeds)  # [B, D']\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "\n",
        "        logit_scale = clip_model.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.T  # [B, 10]\n",
        "\n",
        "        pred = logits_per_image.argmax(dim=-1).item()\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(dataset)\n",
        "\n",
        "\n",
        "print(\"\\nRunning CIFAR-10 zero-shot accuracy tests...\")\n",
        "\n",
        "acc_base = get_accuracy_baseline_cifar(dataset)\n",
        "acc_vtp_07 = get_accuracy_vtp_cifar(dataset, keep_ratio=0.7)\n",
        "acc_vtp_05 = get_accuracy_vtp_cifar(dataset, keep_ratio=0.5)\n",
        "\n",
        "print(\"\\n=== Top-1 Accuracy on 100 CIFAR-10 Images ===\")\n",
        "print(f\"Baseline (no pruning): {acc_base*100:.2f}%\")\n",
        "print(f\"VTP keep_ratio=0.7:    {acc_vtp_07*100:.2f}%\")\n",
        "print(f\"VTP keep_ratio=0.5:    {acc_vtp_05*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Vh07IQPEgwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quantify the impact of our Vision Token Pruning (VTP) scheme on recognition performance, we conduct a zero-shot evaluation on the CIFAR-10 dataset. Following the standard CLIP protocol, we use the text prompts “a photo of a <class>” for the 10 CIFAR-10 classes and evaluate top-1 accuracy on 100 randomly selected test images. As the text encoder and logit computation, we reuse the pretrained CLIP ViT-B/16 model, while the image features are obtained either from the original CLIP vision encoder (baseline) or from our pruned VTP encoder.\n",
        "\n",
        "The results are summarized below:\n",
        "\n",
        "Baseline CLIP (no pruning): 88.00% top-1 accuracy\n",
        "\n",
        "VTP (keep_ratio = 0.7): 10.00% top-1 accuracy\n",
        "\n",
        "VTP (keep_ratio = 0.5): 10.00% top-1 accuracy\n",
        "\n",
        "While the baseline CLIP model achieves a strong 88% accuracy on this small CIFAR-10 subset, our current VTP design causes the performance to collapse to approximately 10%, which is close to random guessing for a 10-class problem. This indicates that, although the pruning mechanism successfully reduces the number of vision tokens and thus the computational cost, it also severely disrupts the learned representation of the CLIP vision transformer."
      ],
      "metadata": {
        "id": "0-HUD5S9ieSw"
      }
    }
  ]
}